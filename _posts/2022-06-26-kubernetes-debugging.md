---
layout: post
title: kubernetes debugging
tags: kubernetes
---

## Set alias for long commands
```
alias k=kubectl
```
### Try it out! 
```
root@k8s-master:~/cka_practice/cert# k get pods
NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-ubuntu-kv1-mgvvz   1/1     Running   0          18d
virt-launcher-vsrx-sriov-qrfxm   2/2     Running   0          19d
```

## Check which file is consuming the most space 

```
du -h <dir> 2>/dev/null | grep '[0-9\.]\+G'
```
## Delete all Evicted pods
This can be used when there are 100's of evicted pods and you want to delete all of them
```
kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod
```

## Advertise out of a specific interface
Typically k8s picks the interface with default IP when creating the cluster using kubeadm init. One can specify the interface in order to advertise the API.
using the below flag in `kubeadm init` 
```
--apiserver-advertise-addresses=<the eth1 ip addr>
```

Example
```
kubeadm init --apiserver-advertise-address=192.169.1.11 --pod-network-cidr=10.244.0.0/16
```

## Label a node
Sometimes if node is not labeled as master pods may not get scheduled. Or if node selectors are being used and label mismatch/not found. you can label the node as below 

### Find labels
```
root@master:~# kubectl get nodes --show-labels
NAME      STATUS   ROLES           AGE     VERSION   LABELS
master    Ready    control-plane   2m43s   v1.25.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready    <none>          116s    v1.25.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux
worker2   Ready    <none>          72s     v1.25.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker2,kubernetes.io/os=linux
```

### Label a node
```
root@master:~# kubectl label nodes master node-role.kubernetes.io/master=
node/master labeled
root@master:~# kubectl label nodes worker1 node-role.kubernetes.io/worker=
node/worker1 labeled
root@master:~# kubectl label nodes worker2 node-role.kubernetes.io/worker=
```

### Verify
```
root@master:~# kubectl get nodes
NAME      STATUS   ROLES                  AGE     VERSION
master    Ready    control-plane,master   4m26s   v1.25.3
worker1   Ready    worker                 3m39s   v1.25.3
worker2   Ready    worker                 2m55s   v1.25.3
```

## Join a cluster
when a cluster is already created and want to join a new node , we would need the token

### Obtain the token
```
$ kubeadm token list
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
jt5hul.iy8150scfrqvf3l3   1h          2022-11-17T19:29:31Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
```
### Create the token
```
$ kubeadm token create --print-join-command
kubeadm join 192.168.2.12:6443 --token nmlxzk.kt5gr2e5pgi8nb4n --discovery-token-ca-cert-hash sha256:3a032c12536c479c959aec0ee2d300a8847d9d0a75f6bd9892be1d61c6afbdb5
```

### Use the token on new worker node
```
user@newworker:~$ sudo kubeadm join 192.168.2.12:6443 --token nmlxzk.kt5gr2e5pgi8nb4n --discovery-token-ca-cert-hash sha256:3a032c12536c479c959aec0ee2d300a8847d9d0a75f6bd9892be1d61c6afbdb5
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: missing optional cgroups: blkio
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

### Verify
```
$ kubectl get nodes
NAME       STATUS     ROLES           AGE     VERSION
master     NotReady   control-plane   22h     v1.25.4
worker1    NotReady   <none>          22h     v1.25.4
worker2    NotReady   <none>          6m18s   v1.25.4
```

## Starting cluster
```
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## Error while dialing dial unix /var/run/dockershim.sock
when a k8s cluster is brought up there are situations where you might have seen the error "Error while dialing dial unix /var/run/dockershim.sock". This would be seen in below 
```
root@jcnr2:~# crictl ps
WARN[0000] runtime connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
E0428 19:02:30.325516   24230 remote_runtime.go:390] "ListContainers with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory\"" filter="&ContainerFilter{Id:,State:&ContainerStateValue{State:CONT
```

To solve, the below, we need to ensure below is added to file `/etc/crictl.yaml`

```
root@vm:~# more /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
```
### Restart service 
```
sudo systemctl restart containerd
```

### Verify
```
root@jcnr2:~# crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID              POD
```

## kubelet failing 
```
root@vm:~# service kubelet status
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset:>
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Fri 2023-04-28 1>
       Docs: https://kubernetes.io/docs/home/
    Process: 25125 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_C>
   Main PID: 25125 (code=exited, status=1/FAILURE)
```

Notice that kubelet is not runing and hence kubeadm init fails . In order to fix sometimes, we might have 

```
sudo kubeadm reset
sudo kubeadm init phase certs all
sudo kubeadm init phase kubeconfig all
sudo kubeadm init phase control-plane all --pod-network-cidr 10.244.0.0/16
sudo sed -i 's/initialDelaySeconds: [0-9][0-9]/initialDelaySeconds: 240/g' /etc/kubernetes/manifests/kube-apiserver.yaml
sudo sed -i 's/failureThreshold: [0-9]/failureThreshold: 18/g'             /etc/kubernetes/manifests/kube-apiserver.yaml
sudo sed -i 's/timeoutSeconds: [0-9][0-9]/timeoutSeconds: 20/g'            /etc/kubernetes/manifests/kube-apiserver.yaml
sudo kubeadm init --v=1 --skip-phases=certs,kubeconfig,control-plane --ignore-preflight-errors=all --pod-network-cidr 10.244.0.0/16
```
## Load images in CRIO 

images loaded should be in .tar format and hence the files are .tgz are used, we need to unzip them.

```
root@jcnr2:~# gunzip crpd.tgz

root@jcnr2:~# ls -l crpd.tar
-rw-r--r-- 1 root root 507279360 Apr 28 19:26 crpd.tar

root@jcnr2:~# ctr -n=k8s.io image import crpd.tar
unpacking docker.io/library/crpd:23.1R1.8 (sha256:bb82530036904d12f19bc2036a3734450712014780e3d27b8de841929a16fc97)...done

root@jcnr2:~# crictl images | grep crpd
docker.io/library/crpd                    23.1R1.8            a1748707249d3       507MB
```

## Pods not scheduled 
This could be because the node may not be tainted. Modify them accordingly to allow pod scheduling since this should be an all in one cluster. The master node itself will run pods.
```
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
```

## Master node labeling
```
kubectl label node ubuntu node-role.kubernetes.io/master=
``` 

## Make file issues when compiling
Error when compiling in ubuntu using Make

```
strip jcnr
make: strip: Command not found
make: *** [Makefile:38: docker-images] Error 127
```

To fix this we need to install below along with installing `make`
```
apt install binutils
```
